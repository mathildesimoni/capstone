{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the links to relevant PDF documents for the issues previously selected\n",
    "# relevant PDF means PDF with potential information about the editorial board\n",
    "# the selection is based on a list of keywords\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = [\"Wiley\", \"Springer\", \"Taylor\", \"Elsevier\"]\n",
    "# publishers = [\"Wiley\"]\n",
    "\n",
    "start_year = 2017\n",
    "end_year = 2021\n",
    "\n",
    "key_words_wiley = [\"issue information\", \"editorial board\", \"editorial-board\", \"publication information\"]\n",
    "key_words_elsevier = [\"issue information\", \"editorial board\", \"editorial-board\", \"editorial advisory board\", \"editors\", \"publication information\"]\n",
    "key_words_springer = key_words_wiley\n",
    "key_words_taylor = key_words_wiley\n",
    "\n",
    "key_words_publishers = {\"Wiley\": key_words_wiley, \"Elsevier\": key_words_elsevier, \"Taylor\": key_words_taylor, \"Springer\": key_words_springer}\n",
    "\n",
    "# key_words = [\"issue information\", \"editors\", \"editor\", \"editorial\", \"Ã©ditorial\"]\n",
    "# key_words = [\"issue information\", \"editorial board\"]\n",
    "\n",
    "for publisher in publishers:\n",
    "    key_words = key_words_publishers[publisher]\n",
    "    print(\"Parsing publisher\", publisher, \"...\")\n",
    "    df = pd.read_csv(\"Journals\" + publisher + \".csv\")\n",
    "    # display(df)\n",
    "    df_null = df.isnull()\n",
    "    # print(df_null)\n",
    "    journal_ids = df[\"Journal_Id\"].to_numpy()\n",
    "    # print(journal_ids)\n",
    "    df_pdf = pd.DataFrame(columns = [\"Journal_Id\", \"Year\", \"Authors\", \"Title\", \"DOI\", \"Link1\", \"Link2\", \"Keyword\"])\n",
    "    # print(df_pdf)\n",
    "    df_got_link_to_pdf = pd.DataFrame(columns = [\"Journal_Id\"])\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        df_got_link_to_pdf[str(year)] = np.nan   \n",
    "    # print(df_got_link_to_pdf)\n",
    "    \n",
    "    for i in range(len(journal_ids)): # for each journal\n",
    "        journal_id = journal_ids[i]\n",
    "        # print(journal_id)\n",
    "        got_links = {\"Journal_Id\": journal_id}\n",
    "        for year in range(start_year, end_year + 1): # for each year\n",
    "            got_link = False # to fill dataframe df_got_link_to_pdf\n",
    "            \n",
    "            # check the journal has an issue for this year\n",
    "            if not df_null.loc[i, \"links_\" + str(year)]:\n",
    "                # open the file and parse it\n",
    "                content = open(publisher.lower() + \"/journal\" + str(journal_id) + \"/year\" + str(year) + \".txt\", \"r\")\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                content.close()\n",
    "                \n",
    "                # start extracting data\n",
    "                body = soup.find(\"tbody\")\n",
    "                if body != None:\n",
    "                    rows = body.find_all(\"tr\")\n",
    "                    for row in rows:\n",
    "                        if row != None:\n",
    "                            cells = row.find_all(\"td\")\n",
    "                            for key_word in key_words:\n",
    "                                # check if one of the keywords is included in the article title\n",
    "                                if key_word in cells[1].text.lower():\n",
    "                                    authors = cells[0].text\n",
    "                                    title = cells[1].text.split(\"DOI: \")[0].strip()\n",
    "                                    doi = cells[1].text.split(\"DOI: \")[1]\n",
    "                                    link1 = cells[4].find_all(\"a\")[0].get(\"href\")\n",
    "                                    link2 = cells[4].find_all(\"a\")[2].get(\"href\")\n",
    "\n",
    "                                    # add info in df_pdf\n",
    "                                    df_pdf = df_pdf.append({\"Journal_Id\": journal_id,\"Year\": year, \"Authors\": authors, \"Title\":title, \"DOI\": doi, \"Link1\": link1, \"Link2\": link2, \"Keyword\": key_word}, ignore_index=True)\n",
    "                                    got_link = True\n",
    "\n",
    "                                    break # to add the row only once\n",
    "            got_links[str(year)] = got_link\n",
    "            \n",
    "        df_got_link_to_pdf = df_got_link_to_pdf.append(got_links, ignore_index=True)\n",
    "    \n",
    "    got_link_all_years = []\n",
    "    for row_index in range(len(df_got_link_to_pdf)):\n",
    "        to_add = False\n",
    "        for j in df_got_link_to_pdf.loc[row_index].values.tolist():\n",
    "            if j == True:\n",
    "                to_add = True\n",
    "                break\n",
    "        got_link_all_years.append(to_add)\n",
    "    df_got_link_to_pdf[\"All_Years\"] = pd.Series(got_link_all_years)\n",
    "    \n",
    "    # save\n",
    "    df_pdf.to_csv(\"Journals\" + publisher + \"PDFs.csv\", index=False)\n",
    "    df_got_link_to_pdf.to_csv(\"Journals\" + publisher + \"GotPDFs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: get list of Elsevier info pages (for Michael)\n",
    "\n",
    "df = pd.read_csv(\"JournalsElsevierPDFs.csv\")\n",
    "df = df[[\"DOI\"]]\n",
    "df.to_csv(\"ElsevierDOIs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
